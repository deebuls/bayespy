<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Bernoulli mixture model &mdash; BayesPy v0.3.6 Documentation</title>
    
    <link rel="stylesheet" href="../_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.3.6',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="BayesPy v0.3.6 Documentation" href="../index.html" />
    <link rel="up" title="Examples" href="examples.html" />
    <link rel="next" title="Hidden Markov model" href="hmm.html" />
    <link rel="prev" title="Gaussian mixture model" href="gmm.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="hmm.html" title="Hidden Markov model"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="gmm.html" title="Gaussian mixture model"
             accesskey="P">previous</a> |</li>
        <li><a href="../index.html">BayesPy v0.3.6 Documentation</a> &raquo;</li>
          <li><a href="examples.html" accesskey="U">Examples</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Bernoulli mixture model</a><ul>
<li><a class="reference internal" href="#data">Data</a></li>
<li><a class="reference internal" href="#model">Model</a></li>
<li><a class="reference internal" href="#inference">Inference</a></li>
<li><a class="reference internal" href="#results">Results</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="gmm.html"
                        title="previous chapter">Gaussian mixture model</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="hmm.html"
                        title="next chapter">Hidden Markov model</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../_sources/examples/bmm.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="bernoulli-mixture-model">
<h1>Bernoulli mixture model<a class="headerlink" href="#bernoulli-mixture-model" title="Permalink to this headline">¶</a></h1>
<p>This example considers data generated from a Bernoulli mixture model.  One
simple example process could be a questionnaire for election candidates.  We
observe a set of binary vectors, where each vector represents a candidate in the
election and each element in these vectors correspond to a candidate&#8217;s answer to
a yes-or-no question.  The goal is to find groups of similar candidates and
analyze the answer patterns of these groups.</p>
<div class="section" id="data">
<h2>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h2>
<p>First, we generate artificial data to analyze.  Let us assume that the
questionnaire contains ten yes-or-no questions.  We assume that there are three
groups with similar opinions.  These groups could represent parties.  These
groups have the following answering patterns, which are represented by vectors
with probabilities of a candidate answering yes to the questions:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">p0</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p1</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p2</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>
</pre></div>
</div>
<p>Thus, the candidates in the first group are likely to answer no to questions 1,
3, 5, 7 and 9, and yes to questions 2, 4, 6, 8, 10.  The candidates in the
second group are likely to answer yes to the last five questions, whereas the
candidates in the third group are likely to answer yes to the first five
questions.  For convenience, we form a NumPy array of these vectors:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">p0</span><span class="p">,</span> <span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">])</span>
</pre></div>
</div>
<p>Next, we generate a hundred candidates.  First, we randomly select the group for
each candidate:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.utils</span> <span class="kn">import</span> <span class="n">random</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">categorical</span><span class="p">([</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<p>Using the group patterns, we generate yes-or-no answers for the candidates:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">z</span><span class="p">])</span>
</pre></div>
</div>
<p>This is our simulated data to be analyzed.</p>
</div>
<div class="section" id="model">
<h2>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h2>
<p>Now, we construct a model for learning the structure in the data.  We have a
dataset of hundred 10-dimensional binary vectors:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">N</span> <span class="o">=</span> <span class="mi">100</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
<p>We will create a Bernoulli mixture model.  We assume that the true number of
groups is unknown to us, so we use a large enough number of clusters:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">K</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
<p>We use the categorical distribution for the group assignments and give the group
assignment probabilities an uninformative Dirichlet prior:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="kn">import</span> <span class="n">Categorical</span><span class="p">,</span> <span class="n">Dirichlet</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">R</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="n">K</span><span class="o">*</span><span class="p">[</span><span class="mf">1e-5</span><span class="p">],</span>
<span class="gp">... </span>              <span class="n">name</span><span class="o">=</span><span class="s">&#39;R&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Z</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">R</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
<span class="gp">... </span>                <span class="n">name</span><span class="o">=</span><span class="s">&#39;Z&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Each group has a probability of a yes answer for each question.  These
probabilities are given beta priors:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="kn">import</span> <span class="n">Beta</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">P</span> <span class="o">=</span> <span class="n">Beta</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
<span class="gp">... </span>         <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">K</span><span class="p">),</span>
<span class="gp">... </span>         <span class="n">name</span><span class="o">=</span><span class="s">&#39;P&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The answers of the candidates are modelled with the Bernoulli distribution:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="kn">import</span> <span class="n">Mixture</span><span class="p">,</span> <span class="n">Bernoulli</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">Mixture</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Bernoulli</span><span class="p">,</span> <span class="n">P</span><span class="p">)</span>
</pre></div>
</div>
<p>Here, <tt class="docutils literal"><span class="pre">Z</span></tt> defines the group assignments and <tt class="docutils literal"><span class="pre">P</span></tt> the answering probability
patterns for each group.  Note how the plates of the nodes are matched: <tt class="docutils literal"><span class="pre">Z</span></tt>
has plates <tt class="docutils literal"><span class="pre">(N,1)</span></tt> and <tt class="docutils literal"><span class="pre">P</span></tt> has plates <tt class="docutils literal"><span class="pre">(D,K)</span></tt>, but in the mixture node the
last plate axis of <tt class="docutils literal"><span class="pre">P</span></tt> is discarded and thus the node broadcasts plates
<tt class="docutils literal"><span class="pre">(N,1)</span></tt> and <tt class="docutils literal"><span class="pre">(D,)</span></tt> resulting in plates <tt class="docutils literal"><span class="pre">(N,D)</span></tt> for <tt class="docutils literal"><span class="pre">X</span></tt>.</p>
</div>
<div class="section" id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h2>
<p>In order to infer the variables in our model, we construct a variational
Bayesian inference engine:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.inference</span> <span class="kn">import</span> <span class="n">VB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">VB</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">P</span><span class="p">)</span>
</pre></div>
</div>
<p>This also gives the default update order of the nodes.  In order to find
different groups, they must be initialized differently, thus we use random
initialization for the group probability patterns:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">P</span><span class="o">.</span><span class="n">initialize_from_random</span><span class="p">()</span>
</pre></div>
</div>
<p>We provide our simulated data:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, we can run inference:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="go">Iteration 1: loglike=-6.872145e+02 (... seconds)</span>
<span class="gp">...</span>
<span class="go">Iteration 17: loglike=-5.236921e+02 (... seconds)</span>
<span class="go">Converged at iteration 17.</span>
</pre></div>
</div>
<p>The algorithm converges in 17 iterations.</p>
</div>
<div class="section" id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h2>
<p>Now we can examine the approximate posterior distribution.  First, let us plot
the group assignment probabilities:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">bayespy.plot</span> <span class="kn">as</span> <span class="nn">bpplt</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">hinton</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/bmm-1.py">Source code</a>, <a class="reference external" href="../examples/bmm-1.png">png</a>, <a class="reference external" href="../examples/bmm-1.hires.png">hires.png</a>, <a class="reference external" href="../examples/bmm-1.pdf">pdf</a>)</p>
<div class="figure">
<img alt="../_images/bmm-1.png" src="../_images/bmm-1.png" />
</div>
<p>This plot shows that there are three dominant groups, which is equal to the true
number of groups used to generate the data.  However, there are still two
smaller groups as the data does not give enough evidence to prune them out.  The
yes-or-no answer probability patterns for the groups can be plotted as:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">hinton</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/bmm-2.py">Source code</a>, <a class="reference external" href="../examples/bmm-2.png">png</a>, <a class="reference external" href="../examples/bmm-2.hires.png">hires.png</a>, <a class="reference external" href="../examples/bmm-2.pdf">pdf</a>)</p>
<div class="figure">
<img alt="../_images/bmm-2.png" src="../_images/bmm-2.png" />
</div>
<p>The three dominant groups have found the true patterns accurately.  The patterns
of the two minor groups some kind of mixtures of the three groups and they exist
because the generated data happened to contain a few samples giving evidence for
these groups.  Finally, we can plot the group assignment probabilities for the
candidates:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">hinton</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/bmm-3.py">Source code</a>, <a class="reference external" href="../examples/bmm-3.png">png</a>, <a class="reference external" href="../examples/bmm-3.hires.png">hires.png</a>, <a class="reference external" href="../examples/bmm-3.pdf">pdf</a>)</p>
<div class="figure">
<img alt="../_images/bmm-3.png" src="../_images/bmm-3.png" />
</div>
<p>This plot shows the clustering of the candidates.  It is possible to use
<a class="reference internal" href="../user_api/generated/generated/bayespy.plot.HintonPlotter.html#bayespy.plot.HintonPlotter" title="bayespy.plot.HintonPlotter"><tt class="xref py py-class docutils literal"><span class="pre">HintonPlotter</span></tt></a> to enable monitoring during the VB iteration by providing
<tt class="docutils literal"><span class="pre">plotter=HintonPlotter()</span></tt> for <tt class="docutils literal"><span class="pre">Z</span></tt>, <tt class="docutils literal"><span class="pre">P</span></tt> and <tt class="docutils literal"><span class="pre">R</span></tt> when creating the nodes.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="hmm.html" title="Hidden Markov model"
             >next</a> |</li>
        <li class="right" >
          <a href="gmm.html" title="Gaussian mixture model"
             >previous</a> |</li>
        <li><a href="../index.html">BayesPy v0.3.6 Documentation</a> &raquo;</li>
          <li><a href="examples.html" >Examples</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2011-2015, Jaakko Luttinen, MIT.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.3.
    </div>
  </body>
</html>