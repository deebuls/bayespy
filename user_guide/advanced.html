<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Advanced topics &mdash; BayesPy v0.4.0 Documentation</title>
    
    <link rel="stylesheet" href="../_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.4.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="BayesPy v0.4.0 Documentation" href="../index.html" />
    <link rel="up" title="User guide" href="user_guide.html" />
    <link rel="next" title="Examples" href="../examples/examples.html" />
    <link rel="prev" title="Examining the results" href="plot.html" /> 
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../examples/examples.html" title="Examples"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="plot.html" title="Examining the results"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">BayesPy v0.4.0 Documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="user_guide.html" accesskey="U">User guide</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Advanced topics</a><ul>
<li><a class="reference internal" href="#gradient-based-optimization">Gradient-based optimization</a></li>
<li><a class="reference internal" href="#collapsed-inference">Collapsed inference</a></li>
<li><a class="reference internal" href="#pattern-search">Pattern search</a></li>
<li><a class="reference internal" href="#deterministic-annealing">Deterministic annealing</a></li>
<li><a class="reference internal" href="#stochastic-variational-inference">Stochastic variational inference</a></li>
<li><a class="reference internal" href="#black-box-variational-inference">Black-box variational inference</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="plot.html"
                        title="previous chapter">Examining the results</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../examples/examples.html"
                        title="next chapter">Examples</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/user_guide/advanced.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="advanced-topics">
<h1>Advanced topics<a class="headerlink" href="#advanced-topics" title="Permalink to this headline">¶</a></h1>
<p>This section contains brief information on how to implement some advanced
methods in BayesPy.  These methods include Riemannian conjugate gradient
methods, pattern search, simulated annealing, collapsed variational inference
and stochastic variational inference.  In order to use these methods properly,
the user should understand them to some extent.  They are also considered
experimental, thus you may encounter bugs or unimplemented features.  In any
case, these methods may provide huge performance improvements easily compared to
the standard VB-EM algorithm.</p>
<div class="section" id="gradient-based-optimization">
<h2>Gradient-based optimization<a class="headerlink" href="#gradient-based-optimization" title="Permalink to this headline">¶</a></h2>
<p>Variational Bayesian learning basically means that the parameters of the
approximate posterior distributions are optimized to maximize the lower bound of
the marginal log likelihood <a class="reference internal" href="../references.html#honkela-2010" id="id1">[3]</a>.  This optimization can be done
by using gradient-based optimization methods.  In order to improve the
gradient-based methods, it is recommended to take into account the information
geometry by using the Riemannian (a.k.a. natural) gradient.  In fact, the
standard VB-EM algorithm is equivalent to a gradient ascent method which uses
the Riemannian gradient and step length 1.  Thus, it is natural to try to
improve this method by using non-linear conjugate gradient methods instead of
gradient ascent.  These optimization methods are especially useful when the
VB-EM update equations are not available but one has to use fixed form
approximation.  But it is possible that the Riemannian conjugate gradient method
improve performance even when the VB-EM update equations are available.</p>
<p>The optimization algorithm in <a class="reference internal" href="../user_api/generated/generated/generated/bayespy.inference.VB.optimize.html#bayespy.inference.VB.optimize" title="bayespy.inference.VB.optimize"><code class="xref py py-func docutils literal"><span class="pre">VB.optimize()</span></code></a> has a simple interface.
Instead of using the default Riemannian geometry, one can use the Euclidean
geometry by giving <code class="code docutils literal"><span class="pre">riemannian=False</span></code>.  It is also possible to choose the
optimization method from gradient ascent (<code class="code docutils literal"><span class="pre">method='gradient'</span></code>) or
conjugate gradient methods (only <code class="code docutils literal"><span class="pre">method='fletcher-reeves'</span></code> implemented at
the moment).  For instance, we could optimize nodes <code class="docutils literal"><span class="pre">C</span></code> and <code class="docutils literal"><span class="pre">X</span></code> jointly
using Euclidean gradient ascent as:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">VB</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">riemannian</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">&#39;gradient&#39;</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="go">Iteration ...</span>
</pre></div>
</div>
<p>Note that this is very inefficient way of updating those nodes (bad geometry and
not using conjugate gradients).  Thus, one should understand the idea of these
optimization methods, otherwise one may do something extremely inefficient.
Most likely this method can be found useful in combination with the advanced
tricks in the following sections.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The Euclidean gradient has not been implemented for all nodes yet.  The
Euclidean gradient is required by the Euclidean geometry based optimization
but also by the conjugate gradient methods in the Riemannian geometry.  Thus,
the Riemannian conjugate gradient may not yet work for all models.</p>
</div>
<p>It is possible to construct custom optimization algorithms with the tools
provided by <a class="reference internal" href="../user_api/generated/generated/bayespy.inference.VB.html#bayespy.inference.VB" title="bayespy.inference.VB"><code class="xref py py-class docutils literal"><span class="pre">VB</span></code></a>.  For instance, <a class="reference internal" href="../user_api/generated/generated/generated/bayespy.inference.VB.get_parameters.html#bayespy.inference.VB.get_parameters" title="bayespy.inference.VB.get_parameters"><code class="xref py py-func docutils literal"><span class="pre">VB.get_parameters()</span></code></a> and
<a class="reference internal" href="../user_api/generated/generated/generated/bayespy.inference.VB.set_parameters.html#bayespy.inference.VB.set_parameters" title="bayespy.inference.VB.set_parameters"><code class="xref py py-func docutils literal"><span class="pre">VB.set_parameters()</span></code></a> can be used to handle the parameters of nodes.
<a class="reference internal" href="../user_api/generated/generated/generated/bayespy.inference.VB.get_gradients.html#bayespy.inference.VB.get_gradients" title="bayespy.inference.VB.get_gradients"><code class="xref py py-func docutils literal"><span class="pre">VB.get_gradients()</span></code></a> is used for computing the gradients of nodes.  The
parameter and gradient objects are not numerical arrays but more complex nested
lists not meant to be accessed by the user.  Thus, for simple arithmetics with
the parameter and gradient objects, use functions <a class="reference internal" href="../user_api/generated/generated/generated/bayespy.inference.VB.add.html#bayespy.inference.VB.add" title="bayespy.inference.VB.add"><code class="xref py py-func docutils literal"><span class="pre">VB.add()</span></code></a> and
<a class="reference internal" href="../user_api/generated/generated/generated/bayespy.inference.VB.dot.html#bayespy.inference.VB.dot" title="bayespy.inference.VB.dot"><code class="xref py py-func docutils literal"><span class="pre">VB.dot()</span></code></a>.  Finally, <a class="reference internal" href="../user_api/generated/generated/generated/bayespy.inference.VB.compute_lowerbound.html#bayespy.inference.VB.compute_lowerbound" title="bayespy.inference.VB.compute_lowerbound"><code class="xref py py-func docutils literal"><span class="pre">VB.compute_lowerbound()</span></code></a> and
<a class="reference internal" href="../user_api/generated/generated/generated/bayespy.inference.VB.has_converged.html#bayespy.inference.VB.has_converged" title="bayespy.inference.VB.has_converged"><code class="xref py py-func docutils literal"><span class="pre">VB.has_converged()</span></code></a> can be used to monitor the lower bound.</p>
</div>
<div class="section" id="collapsed-inference">
<h2>Collapsed inference<a class="headerlink" href="#collapsed-inference" title="Permalink to this headline">¶</a></h2>
<p>The optimization method can be used efficiently in such a way that some of the
variables are collapsed, that is, marginalized out <a class="reference internal" href="../references.html#hensman-2012" id="id2">[1]</a>.  The
collapsed variables must be conditionally independent given the observations and
all other variables.  Probably, one also wants that the size of the marginalized
variables is large and the size of the optimized variables is small.  For
instance, in our PCA example, we could optimize as follows:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">collapsed</span><span class="o">=</span><span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">])</span>
<span class="go">Iteration ...</span>
</pre></div>
</div>
<p>The collapsed variables are given as a list.  This optimization does basically
the following: It first computes the gradients for <code class="docutils literal"><span class="pre">C</span></code> and <code class="docutils literal"><span class="pre">tau</span></code> and takes
an update step using the desired optimization method.  Then, it updates the
collapsed variables by using the standard VB-EM update equations.  These two
steps are taken in turns.  Effectively, this corresponds to collapsing the
variables <code class="docutils literal"><span class="pre">X</span></code> and <code class="docutils literal"><span class="pre">alpha</span></code> in a particular way.  The point of this method is
that the number of parameters in the optimization reduces significantly and the
collapsed variables are updated optimally.  For more details, see
<a class="reference internal" href="../references.html#hensman-2012" id="id3">[1]</a>.</p>
<p>It is possible to use this method in such a way, that the collapsed variables
are not conditionally independent given the observations and all other
variables.  However, in that case, the method does not anymore correspond to
collapsing the variables but just using VB-EM updates after gradient-based
updates.  The method does not check for conditional independence, so the user is
free to do this.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the Riemannian conjugate gradient method has not yet been
implemented for all nodes, it may be possible to collapse those nodes and
optimize the other nodes for which the Euclidean gradient is already
implemented.</p>
</div>
</div>
<div class="section" id="pattern-search">
<h2>Pattern search<a class="headerlink" href="#pattern-search" title="Permalink to this headline">¶</a></h2>
<p>The pattern search method estimates the direction in which the approximate
posterior distributions are updating and performs a line search in that
direction <a class="reference internal" href="../references.html#honkela-2003" id="id4">[4]</a>.  The search direction is based on the difference
in the VB parameters on successive updates (or several updates).  The idea is
that the VB-EM algorithm may be slow because it just zigzags and this can be
fixed by moving to the direction in which the VB-EM is slowly moving.</p>
<p>BayesPy offers a simple built-in pattern search method
<a class="reference internal" href="../user_api/generated/generated/generated/bayespy.inference.VB.pattern_search.html#bayespy.inference.VB.pattern_search" title="bayespy.inference.VB.pattern_search"><code class="xref py py-func docutils literal"><span class="pre">VB.pattern_search()</span></code></a>.  The method updates the nodes twice, measures the
difference in the parameters and performs a line search with a small number of
function evaluations:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">pattern_search</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="go">Iteration ...</span>
</pre></div>
</div>
<p>Similarly to the collapsed optimization, it is possible to collapse some of the
variables in the pattern search.  The same rules of conditional independence
apply as above.  The collapsed variables are given as list:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">pattern_search</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">collapsed</span><span class="o">=</span><span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">])</span>
<span class="go">Iteration ...</span>
</pre></div>
</div>
<p>Also, a maximum number of iterations can be set by using <code class="docutils literal"><span class="pre">maxiter</span></code> keyword
argument.  It is not always obvious whether a pattern search will improve the
rate of convergence or not but if it seems that the convergence is slow because
of zigzagging, it may be worth a try.  Note that the computational cost of the
pattern search is quite high, thus it is not recommended to perform it after
every VB-EM update but every now and then, for instance, after every 10
iterations.  In addition, it is possible to write a more customized VB learning
algorithm which uses pattern searches by using the different methods of
<a class="reference internal" href="../user_api/generated/generated/bayespy.inference.VB.html#bayespy.inference.VB" title="bayespy.inference.VB"><code class="xref py py-class docutils literal"><span class="pre">VB</span></code></a> discussed above.</p>
</div>
<div class="section" id="deterministic-annealing">
<h2>Deterministic annealing<a class="headerlink" href="#deterministic-annealing" title="Permalink to this headline">¶</a></h2>
<p>The standard VB-EM algorithm converges to a local optimum which can often be
inferior to the global optimum and many other local optima.  Deterministic
annealing aims at finding a better local optimum, hopefully even the global
optimum <a class="reference internal" href="../references.html#katahira-2008" id="id5">[5]</a>.  It does this by increasing the weight on the
entropy of the posterior approximation in the VB lower bound.  Effectively, the
annealed lower bound becomes closer to a uniform function instead of the
original multimodal lower bound.  The weight on the entropy is recovered slowly
and the optimization is much more robust to initialization.</p>
<p>In BayesPy, the annealing can be set by using <a class="reference internal" href="../user_api/generated/generated/generated/bayespy.inference.VB.set_annealing.html#bayespy.inference.VB.set_annealing" title="bayespy.inference.VB.set_annealing"><code class="xref py py-func docutils literal"><span class="pre">VB.set_annealing()</span></code></a>.  The
given annealing should be in range <span class="math">\((0,1]\)</span> but this is not validated in
case the user wants to do something experimental.  If annealing is set to 1, the
original VB lower bound is recovered.  Annealing with 0 would lead to an
improper uniform distribution, thus it will lead to errors.  The entropy term is
weighted by the inverse of this annealing term.  An alternative view is that the
model probability density functions are raised to the power of the annealing
term.</p>
<p>Typically, the annealing is used in such a way that the annealing is small at
the beginning and increased after every convergence of the VB algorithm until
value 1 is reached.  After the annealing value is increased, the algorithm
continues from where it had just converged.  The annealing can be used for
instance as:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">beta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">while</span> <span class="n">beta</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">beta</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">beta</span><span class="o">*</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">Q</span><span class="o">.</span><span class="n">set_annealing</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="go">Iteration ...</span>
</pre></div>
</div>
<p>Here, the <code class="docutils literal"><span class="pre">tol</span></code> keyword argument is used to adjust the threshold for
convergence.  In this case, it is a bit larger than by default so the algorithm
does not need to converge perfectly but a rougher convergence is sufficient for
the next iteration with a new annealing value.</p>
</div>
<div class="section" id="stochastic-variational-inference">
<h2>Stochastic variational inference<a class="headerlink" href="#stochastic-variational-inference" title="Permalink to this headline">¶</a></h2>
<p>In stochastic variational inference <a class="reference internal" href="../references.html#hoffman-2013" id="id6">[2]</a>, the idea is to use
mini-batches of large datasets to compute noisy gradients and learn the VB
distributions by using stochastic gradient ascent.  In order for it to be
useful, the model must be such that it can be divided into &#8220;intermediate&#8221; and
&#8220;global&#8221; variables.  The number of intermediate variables increases with the
data but the number of global variables remains fixed.  The global variables are
learnt in the stochastic optimization.</p>
<p>By denoting the data as <span class="math">\(Y=[Y_1, \ldots, Y_N]\)</span>, the intermediate variables
as <span class="math">\(Z=[Z_1, \ldots, Z_N]\)</span> and the global variables as <span class="math">\(\theta\)</span>, the
model needs to have the following structure:</p>
<div class="math">
\[\begin{split}p(Y, Z, \theta) &amp;= p(\theta) \prod^N_{n=1} p(Y_n|Z_n,\theta) p(Z_n|\theta)\end{split}\]</div>
<p>The algorithm consists of three steps which are iterated: 1) a random mini-batch
of the data is selected, 2) the corresponding intermediate variables are updated
by using normal VB update equations, and 3) the global variables are updated
with (stochastic) gradient ascent as if there was as many replications of the
mini-batch as needed to recover the original dataset size.</p>
<p>The learning rate for the gradient ascent must satisfy:</p>
<div class="math">
\[\begin{split}\sum^\infty_{i=1} \alpha_i = \infty \qquad \text{and} \qquad
\sum^\infty_{i=1} \alpha^2 &lt; \infty,\end{split}\]</div>
<p>where <span class="math">\(i\)</span> is the iteration number.  An example of a valid learning
parameter is <span class="math">\(\alpha_i = (\delta + i)^{-\gamma}\)</span>, where <span class="math">\(\delta \geq
0\)</span> is a delay and <span class="math">\(\gamma\in (0.5, 1]\)</span> is a forgetting rate.</p>
<p>Stochastic variational inference is relatively easy to use in BayesPy.  The idea
is that the user creates a model for the size of a mini-batch and specifies a
multiplier for those plate axes that are replicated.  For the PCA example, the
mini-batch model can be costructed as follows.  We decide to use <code class="docutils literal"><span class="pre">X</span></code> as an
intermediate variable and the other variables are global.  The global variables
<code class="docutils literal"><span class="pre">alpha</span></code>, <code class="docutils literal"><span class="pre">C</span></code> and <code class="docutils literal"><span class="pre">tau</span></code> are constructed identically as before.  The
intermediate variable <code class="docutils literal"><span class="pre">X</span></code> is constructed as:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,),</span>
<span class="gp">... </span>                <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
<span class="gp">... </span>                <span class="n">plates_multiplier</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">),</span>
<span class="gp">... </span>                <span class="n">name</span><span class="o">=</span><span class="s">&#39;X&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the plates are <code class="docutils literal"><span class="pre">(1,5)</span></code> whereas they are <code class="docutils literal"><span class="pre">(1,100)</span></code> in the full
model.  Thus, we need to provide a plates multiplier <code class="docutils literal"><span class="pre">(1,20)</span></code> to define how
the plates are replicated to get the full dataset.  These multipliers do not
need to be integers, in this case the latter plate axis is multiplied by
<span class="math">\(100/5=20\)</span>.  The remaining variables are defined as before:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">F</span> <span class="o">=</span> <span class="n">Dot</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;Y&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the plates of <code class="docutils literal"><span class="pre">Y</span></code> and <code class="docutils literal"><span class="pre">F</span></code> also correspond to the size of the
mini-batch and they also deduce the plate multipliers from their parents, thus
we do not need to specify the multiplier here explicitly (although it is ok to
do so).</p>
<p>Let us construct the inference engine for the new mini-batch model:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">VB</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
</pre></div>
</div>
<p>Use random initialization for <code class="docutils literal"><span class="pre">C</span></code> to break the symmetry in <code class="docutils literal"><span class="pre">C</span></code> and <code class="docutils literal"><span class="pre">X</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">C</span><span class="o">.</span><span class="n">initialize_from_random</span><span class="p">()</span>
</pre></div>
</div>
<p>Then, stochastic variational inference algorithm could look as follows:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">ignore_bound_checks</span> <span class="o">=</span> <span class="bp">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">subset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">Y</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span><span class="n">subset</span><span class="p">])</span>
<span class="gp">... </span>    <span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">learning_rate</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mf">2.0</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.7</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">Q</span><span class="o">.</span><span class="n">gradient_step</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="go">Iteration ...</span>
</pre></div>
</div>
<p>First, we ignore the bound checks because they are noisy.  Then, the loop
consists of three parts: 1) Draw a random mini-batch of the data (5 samples from
100).  2) Update the intermediate variable <code class="docutils literal"><span class="pre">X</span></code>.  3) Update global variables
with gradient ascent using a proper learning rate.</p>
</div>
<div class="section" id="black-box-variational-inference">
<h2>Black-box variational inference<a class="headerlink" href="#black-box-variational-inference" title="Permalink to this headline">¶</a></h2>
<p>NOT YET IMPLEMENTED.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../examples/examples.html" title="Examples"
             >next</a> |</li>
        <li class="right" >
          <a href="plot.html" title="Examining the results"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">BayesPy v0.4.0 Documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="user_guide.html" >User guide</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright 2011-2015, Jaakko Luttinen, MIT.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.1.
    </div>
  </body>
</html>